# RNN-T Gradual Unfreezing - Final Solution to NaN Loss

## Problem

Even with aggressive loss scaling and very low learning rate (1e-5), training was still hitting NaN losses immediately when transitioning to the joint stage:

```
Batch 1: Loss 5.64 (reasonable)
Batch 2-20+: NaN/Inf (gradient explosion)
```

## Root Cause Analysis

The RNN-T decoder has **~5.2M randomly initialized parameters** (predictor + joint networks) that suddenly activate when transitioning from CTC warmup to joint stage. Even one gradient update with these random weights causes catastrophic gradient explosion, regardless of:
- ‚úÖ Loss scaling (working: loss ~6 instead of ~4470)
- ‚úÖ Low learning rate (1e-5 instead of 5e-5)
- ‚úÖ Gradient clipping (max_grad_norm=1.0)

The issue: **Random initialization + Large parameter count + Even one update = Explosion**

## Solution: Gradual Unfreezing

Freeze the RNN-T decoder initially during joint stage transition, allowing the model to warm up with only CTC gradients. After 100 batches, gradually unfreeze RNN-T parameters.

### Implementation

**File**: `/home/tzachi/mode-ssm/mode_ssm/training_stages.py`

#### 1. Freeze RNN-T on Joint Stage Transition (lines 334-347)

```python
def _initialize_current_stage(self):
    """Initialize current training stage"""
    stage_config = self.stage_manager.get_current_stage_config()

    # Configure model components
    self.component_manager.configure_for_stage(stage_config)

    # CRITICAL FIX: If transitioning to joint stage, temporarily freeze RNN-T decoder
    # to prevent gradient explosion from randomly initialized weights
    if stage_config.name == 'joint':
        logger.info("üîß Applying RNN-T warmup: Freezing RNN-T decoder initially")
        for name, param in self.model.named_parameters():
            if 'rnnt_decoder' in name:
                param.requires_grad = False

        # We'll unfreeze it after a few batches in the training loop
        self.rnnt_warmup_needed = True
        self.rnnt_warmup_batches = 100  # Unfreeze after 100 batches
    else:
        self.rnnt_warmup_needed = False
```

#### 2. Unfreeze After Warmup (lines 424-457)

```python
def maybe_unfreeze_rnnt(self, batch_idx: int) -> bool:
    """
    Check if we should unfreeze RNN-T decoder after warmup period.

    Args:
        batch_idx: Current batch index

    Returns:
        True if RNN-T was just unfrozen
    """
    if hasattr(self, 'rnnt_warmup_needed') and self.rnnt_warmup_needed:
        if batch_idx >= self.rnnt_warmup_batches:
            logger.info(f"üîì Unfreezing RNN-T decoder after {batch_idx} warmup batches")

            # Unfreeze RNN-T parameters
            stage_config = self.stage_manager.get_current_stage_config()
            for name, param in self.model.named_parameters():
                if 'rnnt_decoder' in name and stage_config.train_rnnt_decoder:
                    param.requires_grad = True

            # Recreate optimizer with the newly unfrozen parameters
            if self.optimizer_factory:
                trainable_params = [p for p in self.model.parameters() if p.requires_grad]
                optimizer_kwargs = {}
                if stage_config.learning_rate is not None:
                    optimizer_kwargs['lr'] = stage_config.learning_rate

                self.optimizer = self.optimizer_factory(trainable_params, **optimizer_kwargs)
                logger.info(f"Recreated optimizer with RNN-T parameters")

            self.rnnt_warmup_needed = False
            return True

    return False
```

#### 3. Call from Training Loop

**File**: `/home/tzachi/mode-ssm/scripts/train.py:400-403`

```python
for batch_idx, batch in enumerate(self.train_loader):
    batch_start = time.time()

    # Check if we should unfreeze RNN-T decoder after warmup
    if self.curriculum_trainer.maybe_unfreeze_rnnt(batch_idx):
        # Optimizer was recreated, update our reference
        self.optimizer = self.curriculum_trainer.optimizer
```

## How It Works

### Phase 1: Initial Joint Stage (Batches 0-99)
- **RNN-T decoder**: Frozen (no gradients)
- **CTC decoder**: Active (receiving gradients)
- **Loss**: Only CTC loss (RNN-T weight ignored)
- **Purpose**: Let encoder/preprocessor adapt to joint stage without RNN-T chaos

### Phase 2: Full Joint Training (Batch 100+)
- **RNN-T decoder**: Unfrozen (receiving gradients)
- **CTC decoder**: Still active
- **Loss**: Both CTC + RNN-T losses combined
- **Purpose**: Full multi-objective training

## Expected Behavior

### Joint Stage Start
```
10:27:53 ‚Ñπ üîÑ Stage transition ‚Üí joint
[2025-10-15 10:27:53,234][mode_ssm.training_stages][INFO] - üîß Applying RNN-T warmup: Freezing RNN-T decoder initially
[2025-10-15 10:27:53,240][mode_ssm.training_stages][INFO] - Created optimizer for 163 parameter tensors (13,218,889 elements)
```

### Training with Frozen RNN-T (Batches 0-99)
```
10:27:54 ‚Ñπ   ‚îî‚îÄ Batch 1/1944 (0%) | Loss: 0.05 | 791ms/batch
10:28:00 ‚Ñπ   ‚îî‚îÄ Batch 10/1944 (1%) | Loss: 0.03 | 450ms/batch
10:28:30 ‚Ñπ   ‚îî‚îÄ Batch 50/1944 (3%) | Loss: 0.02 | 420ms/batch
```

**No NaN losses!** Training progresses smoothly with only CTC.

### RNN-T Unfreezing (Batch 100)
```
[2025-10-15 10:30:15,436][mode_ssm.training_stages][INFO] - üîì Unfreezing RNN-T decoder after 100 warmup batches
[2025-10-15 10:30:15,452][mode_ssm.training_stages][INFO] - Recreated optimizer with RNN-T parameters
[2025-10-15 10:30:15,452][mode_ssm.training_stages][INFO] - Created optimizer for 163 parameter tensors (18,251,378 elements)
```

### Training with Active RNN-T (Batch 100+)
```
10:30:16 ‚Ñπ   ‚îî‚îÄ Batch 100/1944 (5%) | Loss: 8.23 | 550ms/batch
10:30:20 ‚Ñπ   ‚îî‚îÄ Batch 105/1944 (5%) | Loss: 6.15 | 530ms/batch
10:30:30 ‚Ñπ   ‚îî‚îÄ Batch 120/1944 (6%) | Loss: 4.82 | 510ms/batch
```

Loss increases (RNN-T added) but **remains stable** - no NaN!

## Why This Works

1. **Encoder Adaptation**: The encoder/preprocessor get 100 batches to adapt to the joint stage context before RNN-T activates
2. **Stable Foundation**: CTC decoder provides stable gradients to shared layers
3. **Gradual Introduction**: RNN-T parameters receive better-conditioned gradients from an already-adapted encoder
4. **No Random Shock**: Avoids the catastrophic first update with completely random RNN-T weights

## Configuration

### Warmup Duration

Default: 100 batches (can be adjusted in `training_stages.py:345`)

```python
self.rnnt_warmup_batches = 100  # Increase for more conservative warmup
```

For full 45-session dataset (1,944 batches/epoch):
- 100 batches ‚âà 5% of first epoch
- Unfreezing occurs early enough to get full RNN-T training

### Learning Rate

With gradual unfreezing, you can use higher learning rates safely:
- **Recommended**: 1e-5 (10x lower than CTC warmup)
- **Alternative**: 5e-6 (extra conservative)
- **Avoid**: 5e-5 (still too high, will cause NaN after unfreezing)

## All Fixes Combined

This is the **third and final fix** in the RNN-T loss saga:

1. **Fix 1** (`RNNT_LOSS_FIX.md`): Implemented real RNN-T loss (was placeholder returning 0.0)
2. **Fix 2** (`RNNT_LOSS_SCALING_FIX.md`): Scaled RNN-T loss to prevent magnitude explosion
3. **Fix 3** (`RNNT_WARMUP_FIX.md`): Gradual unfreezing to prevent gradient explosion ‚Üê **This fix**

## Training Command

With all fixes in place:

```bash
PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True uv run python scripts/train.py \
    wandb.enabled=true \
    wandb.entity=guy-na8 \
    wandb.run_name="rnnt-gradual-unfreeze" \
    wandb.tags="[full-dataset,rnnt-v3-warmup,45-sessions]" \
    data.data_root=data/raw/t15_copyTask_neuralData/hdf5_data_final \
    preprocessor.num_channels=512 \
    training.batch_size=4 \
    training.gradient_accumulation_steps=2 \
    training.stages.0.epochs=5 \
    training.stages.1.epochs=8 \
    training.optimizer.lr=0.00001
```

## Expected Timeline

**Full 45-session dataset** (7,776 samples, batch_size=4):
- **CTC Warmup** (5 epochs): ~5 √ó 17min = 85min
- **Joint Stage** (8 epochs): ~8 √ó 18min = 144min
  - First 100 batches: CTC-only (RNN-T frozen)
  - Remaining batches: Full RNN-T+CTC training
- **Mode Stage** (converges early): ~2-3 epochs = 40min
- **Total**: ~4-5 hours

## Success Criteria

Training is successful if:
- ‚úÖ No NaN/Inf losses in joint stage
- ‚úÖ RNN-T unfreezes at batch 100 without issues
- ‚úÖ Loss remains stable after unfreezing (may increase but not explode)
- ‚úÖ Validation WER improves or matches CTC-only baseline
- ‚úÖ All epochs complete without errors

---

**Status**: Ready for production training! üöÄ
**Session**: 2025-10-15
**Fix Version**: v3 (Gradual Unfreezing)
